# Distributed Vision-Control System (Face-Locked Servo)

ğŸ“Œ **1. System Description**

The Distributed Vision-Control System is a real-time face-tracking platform built using a distributed architecture.
The system detects a human face through a PC camera and adjusts a servo motor to keep the face centered in the frame. Communication between components is handled using MQTT and WebSocket protocols, alongside direct physical USB connections for advanced face locking.

### ğŸ¯ How It Works

- The **Vision Node (PC)** captures video frames and tracks faces.
- **[Phase 3]** The system recognizes an enrolled face using an `LBPH` model.
- Based on the enrolled face position, it determines movement: `MOVE_LEFT`, `MOVE_RIGHT`, `CENTERED`, `NO_FACE`.
- It calculates an `error` offset for proportional control.
- **[Phase 3]** The error is sent directly via **USB Serial** to an Arduino running a tracking sketch.
- The movement command and error are additionally published via MQTT.
- The **ESP8266** receives the MQTT message and also proportionally rotates the servo (if used in a fully wireless setup).
- The **Backend** relays MQTT updates to the web dashboard using WebSocket.
- The **Dashboard** displays live tracking data, confidence, and system heartbeats.

### ğŸ— System Architecture

```text
[ PC - Vision Node (LBPH Face Recognition) ]
        |                   |
        | USB Serial        | MQTT (vision/teamone/movement & vision/teamone/heartbeat)
        v                   v
[ Arduino Controller ]  [ MQTT Broker ]
        |                   |
        | PWM               | MQTT
        v                   v
[ Tracker Servo ]       [ Backend WebSocket Relay ]
                            |
                            | WebSocket (ws://localhost:9002)
                            v
                        [ Web Dashboard ]
```

### ğŸ”‘ Core Communication Rule

- Vision Node â†’ Publishes via MQTT & transmits USB Serial for local motor tracking.
- Arduino â†’ Parses Serial integer inputs to drive motor physical tracking.
- ESP8266/Backend â†’ Subscribes via MQTT.
- Dashboard â†’ Connects via WebSocket.
- MQTT Broker â†’ Routes messages.

ğŸ“¡ **2. MQTT Topics Used**

Each team must strictly isolate its topic namespace.

`TEAM_NAME = "teamone"`

**Primary Movement Topic**

```text
vision/teamone/movement
```

Message Format Example

```json
{
  "status": "MOVE_LEFT",
  "confidence": 0.9,
  "error": -65,
  "timestamp": 1730000000
}
```

**Heartbeat Topic**

```text
vision/teamone/heartbeat
```

Example:

```json
{
  "node": "pc",
  "status": "ONLINE",
  "timestamp": 1730000000
}
```

âš ï¸ **Important:** Do NOT use wildcard topics. Do NOT subscribe to other teamsâ€™ topics.

ğŸŒ **3. Live Dashboard URL**

The WebSocket server runs locally on: `ws://localhost:9002`

The live dashboard is accessed by opening: `dashboard/index.html`

ğŸ“ **Project Structure**

```text
distributed-vision-control/
â”‚
â”œâ”€â”€ vision-node/
â”‚   â”œâ”€â”€ vision_node.py
â”‚   â””â”€â”€ enroll.py          <-- [New] Run to capture tracking profile
â”‚
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ backend.py
â”‚
â”œâ”€â”€ esp8266/
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ arduino/
â”‚   â””â”€â”€ servo_control.ino  <-- [New] Upload to physical USB-connected Arduino
â”‚
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ index.html
â”‚
â”œâ”€â”€ mosquitto/
â”‚   â””â”€â”€ mosquitto.conf
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

âš™ï¸ **Setup Instructions**

**1ï¸âƒ£ Install Dependencies (PC)**
Make sure to replace basic OpenCV with the `contrib` library for the LBPH recognizer.

```bash
pip uninstall opencv-python
pip install -r requirements.txt
```

**2ï¸âƒ£ Hardware Setup (Arduino Tracking)**

1. Connect an Arduino using a USB cable.
2. Ensure the Serial Port in `vision_node.py` matches the Arduino (e.g. `COM3` or `/dev/ttyUSB0`).
3. Connect the Servo signal pin to Arduino **D9**.
4. Upload `arduino/servo_control.ino` using the Arduino IDE.

**3ï¸âƒ£ Running the System**

**Step 1 â€“ Enroll Your Face**
We use LBPH offline tracking to lock your face identity. Look at the camera.

```bash
cd vision-node
python enroll.py
```

**Step 2 â€“ Start MQTT Broker**

```bash
mosquitto -c mosquitto/mosquitto.conf -v
```

**Step 3 â€“ Start Backend**

```bash
cd backend
python backend.py
```

**Step 4 â€“ Run Vision Node**
Wait for the camera window. When it locks your face, the Arduino servo will physically track you horizontally, and UI updates will broadcast.

```bash
cd vision-node
python vision_node.py
```

**Step 5 â€“ Open Dashboard**
Open `dashboard/index.html` in any modern web browser to view the real-time MQTT feed.

ğŸš€ **Features**

- **Offline Face-Locking**: Uses efficient `LBPH` histograms to recognize and lock only the target user.
- **Low Latency Tracking Engine**: Relays target offsets via USB `pyserial` directly to Arduino.
- Distributed MQTT-based architecture with strict topic isolation.
- Live WebSocket dashboard showing statuses and system heartbeat logs.
